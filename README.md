# kNN
k-近邻算法(kNN)
介绍：KNN是机器学习中最简单易懂的算法，它的适用范围很广，并且在样本量足够大的情况下准确度很高，KNN可以用来进行分类和回归

1、简单的说，k-近邻算法采用测量不同特征值之间的距离进行分类

2、K-近邻的做法是，给定一个训练数据集，当输入一个新的实例时，k-近邻算法会在数据集中找到与该实例最邻近的k个实例，然后新实例所属类别由这k个实例中多数类别来决定，最多的那个类别就作为新实例的类别  
3、不同k值带来的影响      
（1）如果选取较小的k值，那么就意味着我们的整体模型会变的复杂，容易发生过拟合。因为当k较小的时候，很容易学习到噪声数据，那么也就非常容易判定为噪声类别

![image](https://github.com/zjzj1992/kNN/blob/master/images/1.jpg)

（2）如果选取较大的k值，就相当于用较大的领域中的训练数据进行预测，那么这个时候与输入实较远的训练数据也会对预测起作用，从而导致预测发生错误，k值的增大意味着整体模型变的简单。当K=N时，那么无论输入实例是多少，输出的结果都是整个数据中类别最多的那个。这时，模型就简单了，因为根本就不需要训练模型了，只是做了一次哪个类别最多的统计
4、如何选取最优的k值
一般选取一个较小的数值，通常采取交叉验证法来选取最优的k值，选取k值的关键就是要进行调参
5、距离的度量：
选择距离函数应该根据数据的特性和分析的需要而定，但是在一般情况下使用最常用的L2函数(欧氏距离)即可

（1）欧式距离（L2）：![image](https://github.com/zjzj1992/kNN/blob/master/images/oushi.png)  

（2）曼哈顿距离（L1）：![image](https://github.com/zjzj1992/kNN/blob/master/images/manha.png)

![image](https://github.com/zjzj1992/kNN/blob/master/images/juli.jpg)

6、特征归一化的必要性

（1）当特征之间的量纲不同的时候，就需要进行归一化处理了。例如，(身高，鞋码）这样的数据，身高的值要远大于鞋码的值，所以在进行距离度量的时候，结果会更偏向于身高这个特征，导致鞋码特征对分类结果没有什么权重或者贡献，这样特征之间就不是等价的了，最终会导致预测错误

7、概率KNN

（1）有时我们并不想知道一个确切的分类，而只是想知道它属于某个类别的概率是多大，就比如，有一个东西，他可能位于两类物品的分界线，那么不管输出结果的结果是哪个都有可能是错的，但是如果我输出一个“该物品一半可能是这个，一般可能是那个”的话，那么反馈会更有意义

（2）为了获得概率值，我们同样要找出距离最近的k个样本，但是我不是要找出最多的类别，而是计算每个类别在k中的比例，这样的输出就变成了概率的预测了

（3）通过概率KNN算法，可以得到每个特征属于某个类别的概率，通过概率我们可以得到热力图，热力图中颜色越深，代表概率越大

（4）利用概率KNN我们可以通过概率避免一些失误：就比如如果机器预测前方是一只狗熊的概率只要不低于40%，那么就跑，这是一种风控原则

8、kd树
kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构，且kd树是一种二叉树，表示对k维空间的一个划分
当数据量很大的时候，如果还要去计算新样本距离每个点之间的距离的话，那么计算量是非常大的，所以我们采用kd树算法来实现KNN

（1）二叉排序树
在数据结构中，二叉排序树又称二叉查找树或者二叉搜索树。其叉排序树具有的性质：
    若它的左子树不空，则左子树上所有的结点的值均小于它的根节点的值
    若它的右子树不空，则右子树上所有结点的值均大于它的根节点的值
    左右子树也是二叉排序树

![image](https://github.com/zjzj1992/kNN/blob/master/images/erchashu.bmp)

二叉树中，有两种构建方法，一种是中位数构建，一种是顺序构建。但是按照顺序构建的话，检索效率会下降
（2） kd树
kd树与二叉树的基本思想是类似的，不同点在于，kd树中，每一个节点表示的是一个样本，通过选择样本中的某一维特征，将样本划分到不同的节点中，比如，有样本{(7,2),(5,4),(9,6)}，考虑数据的第一维，那么根节点是(7,2)，其中(5,4)中的第一维的值小于根节点的第一维，所以该点进入左子树；然后是(9,6)，这个样本的第一维是9，值是大于7的，所有，进入右子树：

![image](https://github.com/zjzj1992/kNN/blob/master/images/kd.png)

在kd树的操作中，主要包括kd树的建立和kd树的检索两部分
（3）首先是kd树的建立
构建kd树相当于不断地用垂直于坐标轴地超平面将k维空间切分成一系列地k维超矩阵区域。选择划分节点地方式主要有两种：一个是按顺序选择，一个是按照数据地中位数进行划分，按照中位数进行划分是为了防止出现只有左子树或只有右子树地情况，，而如果按照中位数进行划分的话，那么构建出来的kd树将是平衡的
（李航的《统计机器学习》中提到了，平衡的kd树的效率未必是最优的）
（4）kd树的检索
与二叉树排序树一样，在kd树种，将样本划分到不同的空间中，在查找的过程中，由于查找某些情况下的时候仅需要查找部分的空间，所以这个查找的过程节省了对数据点的搜索过程，对于kd树的检索，具体过程是：
第一步：先从根节点出发，将待检测的样本划分到对应的区域内，并且在查找的过程中，会将根节点到叶节点的这个查找序列会存储到栈中
第二步：用栈顶层的元素与待检测的样本之间的距离作为最短距离
第三步：向上回溯，查找父节点，若父节点与待检测样本之间的距离要小于当前的最短距离的话，则将会替换当前的最短距离；然后以待检测的样本作为圆心，并以最短距离作为半径画一个圆，若圆与父节点所在的平面相割，那么就证明可能还有与待测样本之间距离更短的节点，所以还需要将父节点的另一颗子树进栈，重新计算另一颗子树中节点与待测样本之间的距离，然后再次进行距离的比较
第四步：直到回溯到根节点为止
在第三步中，若找到了还有比待测样本之间更短的节点，那么就需要将父节点的另一颗子树进栈，所以，若需要进栈——的子树中有很多节点，则需要根据需要比较元素的大小，将直到叶节点的所有节点都进栈
例子：

![image](https://github.com/zjzj1992/kNN/blob/master/images/tu1.png)


                                   图一

![image](https://github.com/zjzj1992/kNN/blob/master/images/tu2.png)

                                    图二
定义一个新的样本点(2.1,3.1)，然后通过二叉树搜索，顺着搜索路径查找，对于新样本点的第一维的值是2.1，该值小于根节点中第一维的数值，所以进入左子树，然后使用同样的方式再次进行比较，依然进入左子树，此时就找到了新样本划分的对应区域了，并且，再找到新样本对应的划分区域后，还要存储对应的进栈序列{(7,2),(5,4),(2,3)}；然后计算新样本与该节点中的样本的距离，是0.1414。然后以新样本为圆心，以半径为0.14画一个圆，此时该圆与(2,3)所在的平面相割，所以需要检索以(2,3)为根节点的另一颗子树，但是该kd树中(2,3)下面是没有子节点的，所以我们继续向上回溯，找到了对应的父节点(5,4)，然后计算该点到待测样本的距离，是3.04，如果计算的距离比之前的更短，那么就代替当前的最短距离，但此时3.04 > 0.14，所以不需要替换，然后还是以0.14为半径画圆，该圆并没有与(5,4)的平面相割，所以不需要进入(5,4)节点进行搜索了，然后继续向上回溯，这次计算根节点(7,2)到待测样本的距离，是5.02。距离更长了，所以不需要搜索。最后返回最近邻的点，就是(2,3)。

总结：从root节点开始，使用DFS搜索到叶子节点，同时再stack中按顺序存储已经访问的节点。
如果搜索到了叶子节点，那么就将该叶子节点作为最近邻的节点。
然后通过stack回溯
如果当前点的距离比最近邻点的距离更近的话，就更新最近邻节点
然后检查以待测样本为圆心，最短距离为半径的圆是否和父节点的超平面相交
如果相交的话，就证明有可能还有更短的距离，所以必须到父节点的另一侧，用DFS方法，继续检查最近邻点
但是如果不相交的话，就继续按顺序向上回溯，父节点的另一次就不再考虑了
当搜索到root节点时，就可以返回最近邻节点了

9、优点：理论简单、容易实现、精度高、没有估计参数、对异常值不敏感、既可以用来做分类也可以用来做回归、适用于多分类任务
     缺点：计算量大(对每个样本分类时都需要重新进行一次全局运算)、空间复杂度高、需要大量的存储空间、样本不平衡的话，预测结论会有偏差(一些类的样本很多，一些类的样本比较少)

10、伪代码：

（1）计算已知类别属性的数据集中的点与未知点之间的距离；

（2）按照距离递增次序排序；

（3）选取与当前点距离最小的k个点；

（4）确定前k个点所在类别的出现次数；

（5）返回前k个点出现次数最高的类别作为当前点预测的分类。
